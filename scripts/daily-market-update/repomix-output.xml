This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
src/
  core/
    config.py
    logging_config.py
    supabase_client.py
    util.py
  models/
    data_models.py
  services/
    data_fetcher.py
    data_saver.py
    ticker_fetcher.py
    ticker_processor.py
  lambda_handler.py
tests/
  test_ticker_process.py
.gitignore
fetch_yfinance_data.py
package.json
pytest.ini
README.md
requirements.txt
serverless.yml
test.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/core/config.py">
import os
from dotenv import load_dotenv

# Load environment variables from .env file locally (optional for development)
load_dotenv()

# Configuration using environment variables
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError(
        "SUPABASE_URL and SUPABASE_KEY must be set in environment variables"
    )
</file>

<file path="src/core/logging_config.py">
import logging
import sys
from typing import Optional


def setup_logging(
        name: str = "daily-market-update",
        level: int = logging.INFO,
        log_format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        extra_handlers: Optional[list] = None
) -> logging.Logger:
    """
    Configure and return a logger with structured logging.

    Args:
        name: Name of the logger (default: "daily-market-update").
        level: Logging level (default: INFO).
        log_format: Format string for log messages.
        extra_handlers: Additional handlers to add (optional).

    Returns:
        Configured logger instance.
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Avoid duplicate handlers if already configured
    if not logger.handlers:
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)

        # Formatter
        formatter = logging.Formatter(log_format)
        console_handler.setFormatter(formatter)

        # Add handlers
        logger.addHandler(console_handler)

        # Add extra handlers if provided
        if extra_handlers:
            for handler in extra_handlers:
                logger.addHandler(handler)

    # Prevents propagation to root logger
    logger.propagate = False

    return logger
</file>

<file path="src/core/supabase_client.py">
from supabase import create_client
from src.core.logging_config import setup_logging


logger = setup_logging(name="ticker_processor")


class SupabaseClient:
    def __init__(self, url, key):
        self.url = url
        self.key = key
        self.client = self._create_client()

    def _create_client(self):
        try:
            return create_client(self.url, self.key)
        except Exception as e:
            logger.error(f"Failed to create Supabase client: {e}")
            raise

    def get_client(self):
        return self.client
</file>

<file path="src/core/util.py">
from typing import Any, Callable, Optional


def get_value(
        data: dict,
        key: str,
        cast_type: Optional[Callable] = None,
        default: Any = None
) -> Any:
    """
    Safely retrieves a value from a dict, optionally casting it to a specified type.

    Args:
        data: The dictionary to extract the value from.
        key: The key to look up.
        cast_type: Optional type to cast the value to (e.g., float, int).
        default: Value to return if key is missing or value is None.

    Returns:
        The value (cast if specified), or default if not found/None.
    """
    value = data.get(key)
    if value is not None and cast_type:
        try:
            return cast_type(value)
        except (ValueError, TypeError) as e:
            return default
    return value if value is not None else default
</file>

<file path="src/models/data_models.py">
from pydantic import BaseModel, Field, validator, ConfigDict
from datetime import datetime, date
from typing import Optional, List, Dict
import pandas as pd

model_config = ConfigDict(
    arbitrary_types_allowed=True,
    extra="forbid",  # Prevents unexpected fields
)


class PriceData(BaseModel):
    ticker_id: str
    date: str = Field(..., pattern=r"^\d{4}-\d{2}-\d{2}$")  # YYYY-MM-DD
    open_price: Optional[float] = None
    high_price: Optional[float] = None
    low_price: Optional[float] = None
    close_price: Optional[float] = None
    dividends: Optional[float] = None
    stock_splits: Optional[float] = None
    volume: Optional[int] = None
    updated_at: str = Field(default_factory=lambda: datetime.now().isoformat())

    @validator("date", pre=True)
    def parse_date(cls, v):
        if isinstance(v, pd.Timestamp):
            return v.strftime("%Y-%m-%d")
        return v


class TickerInfo(BaseModel):
    long_business_summary: Optional[str] = None
    category: Optional[str] = None
    region: Optional[str] = None
    quote_type: str = Field(default="EQUITY")
    backfill: bool = False
    industry: Optional[str] = None
    sector: Optional[str] = None
    dividend_amount: Optional[float] = None
    updated_at: str = Field(default_factory=lambda: datetime.now().isoformat())


class FinanceData(BaseModel):
    ticker_id: str
    date: str = Field(default_factory=lambda: date.today().strftime("%Y-%m-%d"))
    regular_market_price: Optional[float] = None
    regular_market_change_percent: Optional[float] = None
    market_cap: Optional[int] = None  # Changed to int to match bigint
    dividend_yield: Optional[float] = None
    fifty_two_week_low: Optional[float] = None
    fifty_two_week_high: Optional[float] = None
    fifty_day_average: Optional[float] = None
    two_hundred_day_average: Optional[float] = None
    trailing_pe: Optional[float] = None
    total_assets: Optional[int] = None
    nav_price: Optional[float] = None
    yield_: Optional[float] = Field(None, alias="yield")  # Avoid Python keyword
    ytd_return: Optional[float] = None
    beta3year: Optional[float] = None
    fund_family: Optional[str] = None
    fund_inception_date: Optional[str] = None
    legal_type: Optional[str] = None
    three_year_average_return: Optional[float] = None
    five_year_average_return: Optional[float] = None
    net_expense_ratio: Optional[float] = None
    shares_outstanding: Optional[int] = None  # Changed to int to match bigint
    trailing_three_month_returns: Optional[float] = None
    trailing_three_month_nav_returns: Optional[float] = None
    updated_at: str = Field(default_factory=lambda: datetime.now().isoformat())


class CalendarEvent(BaseModel):
    ticker_id: str
    date: str = Field(..., pattern=r"^\d{4}-\d{2}-\d{2}$")
    event_type: str  # e.g., "dividend", "ex_dividend", "earnings"
    created_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    updated_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    earnings_dates: Optional[List[str]] = None  # For earnings events
    earnings_high: Optional[float] = None
    earnings_low: Optional[float] = None
    earnings_average: Optional[float] = None
    revenue_high: Optional[int] = None
    revenue_low: Optional[int] = None
    revenue_average: Optional[int] = None
</file>

<file path="src/services/data_fetcher.py">
import yfinance as yf
from datetime import date, timedelta
from src.core.logging_config import setup_logging


logger = setup_logging(name="ticker_processor")
class DataFetcher:
    def __init__(self):
        self.exchange_map = {
            "NASDAQ": "",
            "NYSE": "",
            "AMEX": ".AM",
            "TSX": ".TO",
            "LSE": ".L",
            "FRA": ".F",
            "TSE": ".T",
            "HKEX": ".HK",
            "SSE": ".SS",
        }

    def format_yahoo_ticker(self, symbol, exchange):
        if not exchange:
            return symbol
        suffix = self.exchange_map.get(exchange.upper(), "")
        if not suffix and exchange.upper() not in self.exchange_map:
            logger.warning(
                f"Unknown exchange {exchange} for {symbol}; using symbol as is"
            )
        return f"{symbol}{suffix}"

    def fetch_stock_data(self, symbol, exchange, start_date, end_date):
        yahoo_ticker = self.format_yahoo_ticker(symbol, exchange)
        logger.debug(
            f"Fetching data for {symbol} ({yahoo_ticker}) from {start_date} to {end_date}"
        )
        try:
            ticker = yf.Ticker(yahoo_ticker)
            data = ticker.history(start=start_date, end=end_date, auto_adjust=True)
            info = ticker.info if hasattr(ticker, "info") else {}
            return data, info, ticker
        except Exception as e:
            logger.error(f"Failed to fetch data for {symbol} ({yahoo_ticker}): {e}")
            return None, {}, None

    def determine_start_date(self, last_update_date, backfill=False):
        today = date.today()
        if backfill:
            return date(2020, 1, 1)  # Start from 2020 for backfill
        return (
            last_update_date + timedelta(days=1)
            if last_update_date
            else today - timedelta(days=30)
        )
</file>

<file path="src/services/data_saver.py">
from datetime import datetime, date
from typing import Optional, Dict
import pandas as pd
from src.models.data_models import PriceData, TickerInfo, FinanceData, CalendarEvent
from pydantic import ValidationError
from src.core.logging_config import setup_logging
from src.core.util import get_value

logger = setup_logging(name="data_saver")


class DataSaver:
    def __init__(self, supabase_client):
        self.supabase = supabase_client

    def should_update(
        self, last_update_date: Optional[date], threshold_days: int = 1
    ) -> bool:
        if not last_update_date:
            return True
        return (date.today() - last_update_date).days >= threshold_days

    def get_last_update_date(self, ticker_id: str, table_name: str) -> Optional[date]:
        try:
            response = (
                self.supabase.table(table_name)
                .select("date")
                .eq("ticker_id", ticker_id)
                .order("date", desc=True)
                .limit(1)
                .execute()
            )
            if response.data and len(response.data) > 0:
                logger.debug(
                    f"Last update date response for {ticker_id} in {table_name}: {response.data}"
                )
                date_str = response.data[0]["date"]
                if date_str is not None:
                    return datetime.strptime(date_str, "%Y-%m-%d").date()
                else:
                    logger.warning(
                        f"No valid date found for ticker {ticker_id} in {table_name}, date is None"
                    )
                    return None
            logger.debug(f"No entries found for ticker {ticker_id} in {table_name}")
            return None
        except Exception as e:
            logger.error(
                f"Failed to get last update date for ticker {ticker_id} in {table_name}: {e}"
            )
            return None

    def save_price_data(self, ticker_id: str, symbol: str, data: pd.DataFrame) -> int:
        if data is None or data.empty:
            logger.debug(f"No price data for {symbol}")
            return 0

        try:
            price_data_list = [
                PriceData(
                    ticker_id=ticker_id,
                    date=index,
                    open_price=float(row["Open"]) if not pd.isna(row["Open"]) else None,
                    high_price=float(row["High"]) if not pd.isna(row["High"]) else None,
                    low_price=float(row["Low"]) if not pd.isna(row["Low"]) else None,
                    close_price=float(row["Close"])
                    if not pd.isna(row["Close"])
                    else None,
                    dividends=float(row["Dividends"])
                    if not pd.isna(row["Dividends"])
                    else None,
                    stock_splits=float(row["Stock Splits"])
                    if not pd.isna(row["Stock Splits"])
                    else None,
                    volume=int(row["Volume"]) if not pd.isna(row["Volume"]) else None,
                ).dict(exclude_none=True)
                for index, row in data.iterrows()
            ]

            try:
                response = (
                    self.supabase.table("historical_prices")
                    .upsert(price_data_list, on_conflict="ticker_id,date")
                    .execute()
                )
            except Exception as e:
                logger.error(
                    f"Error saving to supabse for {symbol}", extra={"error": str(e)}
                )

            logger.info(f"Saved {len(price_data_list)} price records for {symbol}")
            return len(price_data_list)
        except ValidationError as e:
            logger.error(f"Validation error for {symbol}", extra={"error": str(e)})
            return 0
        except Exception as e:
            logger.exception(f"Failed to save price data for {symbol}")
            return 0

    def update_ticker_info(
        self, ticker_id: str, symbol: str, info: Dict, backfill: bool = False
    ) -> bool:
        if not info:
            return False

        ticker_info = TickerInfo(
            long_business_summary=info.get("longBusinessSummary"),
            category=info.get("category"),
            region=info.get("region"),
            quote_type=info.get("quoteType", "EQUITY"),
            backfill=False,
            industry=info.get("industryKey") if backfill else None,
            sector=info.get("sectorKey") if backfill else None,
            dividend_amount=info.get("lastDividendValue") if backfill else None,
        ).dict(exclude_none=True)

        if not ticker_info:
            return False
        try:
            self.supabase.table("tickers").update(ticker_info).eq(
                "id", ticker_id
            ).execute()
            return True
        except Exception as e:
            logger.error(f"Failed to update tickers table for {symbol}: {e}")
            return False

    def save_finance_data(self, ticker_id, symbol, info):
        """Saves finance data to yh_finance_daily in a batched request."""
        if not info:
            logger.debug(f"No finance data available for {symbol}")
            return False

        quote_type = info.get("quoteType", "EQUITY")
        field_configs = {
            "regular_market_price": {"types": ["EQUITY", "ETF"], "cast": float},
            "regular_market_change_percent": {
                "types": ["EQUITY", "ETF"],
                "cast": float,
            },
            "market_cap": {"types": ["EQUITY", "ETF"], "cast": int},
            "dividend_yield": {"types": None, "cast": float},  # No type restriction
            "fifty_two_week_low": {"types": ["EQUITY", "ETF"], "cast": float},
            "fifty_two_week_high": {"types": ["EQUITY", "ETF"], "cast": float},
            "fifty_day_average": {"types": ["EQUITY", "ETF"], "cast": float},
            "two_hundred_day_average": {"types": ["EQUITY", "ETF"], "cast": float},
            "trailing_pe": {"types": ["EQUITY"], "cast": float},
            "total_assets": {"types": ["MUTUALFUND", "ETF"], "cast": int},
            "nav_price": {"types": ["MUTUALFUND", "ETF"], "cast": float},
            "yield_": {"types": ["MUTUALFUND", "ETF"], "cast": float},
            "ytd_return": {"types": None, "cast": float},
            "beta3year": {
                "types": None,
                "cast": float,
                "key": "beta",
            },  # Maps to "beta" in info
            "fund_family": {"types": ["MUTUALFUND", "ETF"], "cast": str},
            "fund_inception_date": {
                "types": ["MUTUALFUND", "ETF"],
                "cast": lambda x: datetime.fromtimestamp(x).strftime("%Y-%m-%d")
                if x
                else None,
            },
            "legal_type": {"types": ["MUTUALFUND", "ETF"], "cast": str},
            "three_year_average_return": {
                "types": ["MUTUALFUND", "ETF"],
                "cast": float,
            },
            "five_year_average_return": {"types": ["MUTUALFUND", "ETF"], "cast": float},
            "net_expense_ratio": {"types": ["MUTUALFUND", "ETF"], "cast": float},
            "shares_outstanding": {"types": ["EQUITY", "ETF"], "cast": int},
            "trailing_three_month_returns": {
                "types": ["MUTUALFUND", "ETF"],
                "cast": float,
            },
            "trailing_three_month_nav_returns": {
                "types": ["MUTUALFUND", "ETF"],
                "cast": float,
            },
        }

        finance_kwargs = {"ticker_id": ticker_id}
        for field, config in field_configs.items():
            if config["types"] is None or quote_type in config["types"]:
                key = config.get(
                    "key", field
                )  # Use "key" if specified, else field name
                finance_kwargs[field] = get_value(info, key, cast_type=config["cast"])

        finance_data = FinanceData(**finance_kwargs).dict(exclude_none=True)

        if len(finance_data) <= 3:  # Only ticker_id, date, and updated_at
            logger.debug(f"Insufficient finance data for {symbol}")
            return False

        try:
            self.supabase.table("yh_finance_daily").upsert(
                [finance_data],
                on_conflict=["ticker_id"],
            ).execute()
            return True
        except Exception as e:
            logger.error(f"Failed to update yh_finance_daily for {symbol}: {e}")
            return False

    def save_calendar_events(self, ticker_id, symbol, ticker):
        """Saves calendar events from yfinance to the calendar_events table."""
        if not hasattr(ticker, "calendar"):
            logger.debug(f"No calendar attribute available for {symbol}")
            return False

        try:
            calendar = ticker.calendar
        except Exception as e:
            logger.warning(f"Failed to fetch calendar data for {symbol}: {e}")
            calendar = None

        if calendar is None or (isinstance(calendar, dict) and not calendar):
            logger.debug(f"No calendar data available or empty for {symbol}")
            return False

        events_data = []

        # Handle Dividend Date
        if "Dividend Date" in calendar and calendar["Dividend Date"]:
            events_data.append(
                CalendarEvent(
                    ticker_id=str(ticker_id),
                    date=calendar["Dividend Date"].strftime("%Y-%m-%d"),
                    event_type="dividend",
                ).dict(exclude_none=True)
            )

        if "Ex-Dividend Date" in calendar and calendar["Ex-Dividend Date"]:
            events_data.append(
                CalendarEvent(
                    ticker_id=str(ticker_id),
                    date=calendar["Ex-Dividend Date"].strftime("%Y-%m-%d"),
                    event_type="ex_dividend",
                ).dict(exclude_none=True)
            )

        if "Earnings Date" in calendar and calendar["Earnings Date"]:
            earnings_dates = calendar["Earnings Date"]
            if isinstance(earnings_dates, list) and earnings_dates:
                earnings_dates_str = [
                    d.strftime("%Y-%m-%d") for d in earnings_dates if d
                ]
                if earnings_dates_str:
                    earnings_event = CalendarEvent(
                        ticker_id=str(ticker_id),
                        date=earnings_dates_str[0],
                        event_type="earnings",
                        earnings_dates=earnings_dates_str,
                        earnings_high=get_value(calendar, "Earnings High", float),
                        earnings_low=get_value(calendar, "Earnings Low", float),
                        earnings_average=get_value(calendar, "Earnings Average", float),
                        revenue_high=get_value(calendar, "Revenue High", int),
                        revenue_low=get_value(calendar, "Revenue Low", int),
                        revenue_average=get_value(calendar, "Revenue Average", int),
                    ).dict(exclude_none=True)
                    events_data.append(earnings_event)

        if not events_data:
            logger.debug(f"No valid calendar events to save for {symbol}")
            return False

        try:
            self.supabase.table("calendar_events").upsert(
                events_data, on_conflict="ticker_id,date,event_type"
            ).execute()
            logger.info(
                f"Updated calendar_events table for {symbol} with {len(events_data)} events"
            )
            return True
        except Exception as e:
            logger.error(f"Failed to save calendar events for {symbol}: {e}")
            return False

    def save_fund_top_holdings(
        self, ticker_id, symbol, ticker, data_key="top_holdings"
    ):
        """Saves fund top holdings data from yfinance."""
        logger.debug(
            f"Attempting to save {data_key} for {symbol}",
            extra={"table": "fund_top_holdings"},
        )

        if not hasattr(ticker, "funds_data"):
            logger.error(f"No funds_data attribute available for {symbol}")
            return False

        data = getattr(ticker.funds_data, data_key, None)
        if data is None or not isinstance(data, pd.DataFrame):
            logger.error(f"No valid {data_key} DataFrame available for {symbol}")
            return False

        logger.debug(f"Raw {data_key} data for {symbol}", extra={"data": str(data)})
        upsert_data = []

        try:
            for index, row in data.iterrows():
                upsert_data.append(
                    {
                        "ticker_id": ticker_id,
                        "holding_symbol": index,
                        "holding_name": row["Name"],
                        "weight": float(row["Holding Percent"]) * 100,
                        "date": date.today().strftime("%Y-%m-%d"),
                        "updated_at": datetime.now().isoformat(),
                    }
                )
        except KeyError as e:
            logger.error(
                f"Error processing top holdings DataFrame for {symbol}: Missing key {e}"
            )
            return False

        return self._upsert_data(
            upsert_data,
            "fund_top_holdings",
            symbol,
            data_key,
            on_conflict="ticker_id,holding_symbol",
        )

    def save_fund_sector_weightings(
        self, ticker_id, symbol, ticker, data_key="sector_weightings"
    ):
        """Saves fund sector weightings data from yfinance."""
        logger.debug(
            f"Attempting to save {data_key} for {symbol}",
            extra={"table": "fund_sector_weightings"},
        )

        if not hasattr(ticker, "funds_data"):
            logger.error(f"No funds_data attribute available for {symbol}")
            return False

        data = getattr(ticker.funds_data, data_key, None)
        if data is None or not isinstance(data, dict):
            logger.error(f"No valid {data_key} dictionary available for {symbol}")
            return False

        logger.debug(f"Raw {data_key} data for {symbol}", extra={"data": str(data)})
        upsert_data = []

        try:
            for sector_name, weight in data.items():
                upsert_data.append(
                    {
                        "ticker_id": ticker_id,
                        "sector_name": sector_name.replace("_", "-"),
                        "weight": float(weight) * 100,
                        "date": date.today().strftime("%Y-%m-%d"),
                        "updated_at": datetime.now().isoformat(),
                    }
                )
        except Exception as e:
            logger.error(f"Error processing sector weightings for {symbol}: {e}")
            return False

        return self._upsert_data(
            upsert_data,
            "fund_sector_weightings",
            symbol,
            data_key,
            on_conflict="ticker_id,sector_name",
        )

    def save_fund_asset_classes(
        self, ticker_id, symbol, ticker, data_key="asset_allocation"
    ):
        """Saves fund asset classes data from yfinance."""
        logger.debug(
            f"Attempting to save {data_key} for {symbol}",
            extra={"table": "fund_asset_classes"},
        )

        if not hasattr(ticker, "funds_data"):
            logger.error(f"No funds_data attribute available for {symbol}")
            return False

        data = getattr(ticker.funds_data, data_key, None)
        if data is None or not isinstance(data, dict):
            logger.error(f"No valid {data_key} dictionary available for {symbol}")
            return False

        logger.debug(f"Raw {data_key} data for {symbol}", extra={"data": str(data)})
        upsert_data = []

        try:
            for asset_class, weight in data.items():
                upsert_data.append(
                    {
                        "ticker_id": ticker_id,
                        "asset_class": asset_class,
                        "weight": float(weight) * 100,
                        "date": date.today().strftime("%Y-%m-%d"),
                        "updated_at": datetime.now().isoformat(),
                    }
                )
        except Exception as e:
            logger.error(f"Error processing asset classes for {symbol}: {e}")
            return False

        return self._upsert_data(
            upsert_data,
            "fund_asset_classes",
            symbol,
            data_key,
            on_conflict="ticker_id,asset_class",
        )

    def _upsert_data(self, upsert_data, table_name, symbol, data_key, on_conflict):
        """Helper method to handle the database upsert operation."""
        if not upsert_data:
            logger.info(
                f"No valid {data_key} data to save for {symbol} after processing"
            )
            return False

        logger.debug(
            f"Preparing to upsert {len(upsert_data)} records",
            extra={"table": table_name, "data_sample": upsert_data[:1]},
        )

        try:
            self.supabase.table(table_name).upsert(
                upsert_data, on_conflict=on_conflict
            ).execute()
            logger.info(
                f"Successfully saved {len(upsert_data)} {data_key} records for {symbol}"
            )
            return True
        except Exception as e:
            logger.exception(f"Database upsert failed for {symbol} {data_key}")
            return False
</file>

<file path="src/services/ticker_fetcher.py">
from src.core.logging_config import setup_logging


logger = setup_logging(name="ticker_fetcher")

class TickerFetcher:
    def __init__(self, supabase_client):
        self.supabase = supabase_client

    def fetch_tickers(self):
        try:
            response = (
                self.supabase.table("tickers")
                .select("id, symbol, exchange, backfill")
                .execute()
            )
            logger.debug(f"Fetched {len(response.data)} tickers from Supabase")
            return response.data
        except Exception as e:
            logger.error(f"Failed to fetch tickers from Supabase: {e}")
            return []
</file>

<file path="src/services/ticker_processor.py">
from datetime import date, timedelta
from src.services.data_fetcher import DataFetcher
from src.services.data_saver import DataSaver
from src.core.logging_config import setup_logging


logger = setup_logging(name="ticker_processor")


class TickerProcessor:
    def __init__(self, supabase_client):
        self.data_fetcher = DataFetcher()
        self.data_saver = DataSaver(supabase_client)

    def process_ticker(self, ticker):
        symbol = ticker["symbol"]
        ticker_id = ticker["id"]
        exchange = ticker.get("exchange", "")
        backfill = ticker.get("backfill", False)
        logger.info(f"Processing ticker: {symbol} (Backfill: {backfill})")

        updates = set()
        today = date.today()
        end_date = today + timedelta(days=1)

        last_price_update = self.data_saver.get_last_update_date(
            ticker_id, "historical_prices"
        )
        last_finance_update = self.data_saver.get_last_update_date(
            ticker_id, "yh_finance_daily"
        )
        last_calendar_update = self.data_saver.get_last_update_date(
            ticker_id, "calendar_events"
        )

        start_date = self.data_fetcher.determine_start_date(last_price_update, backfill)
        logger.info(
            f"Processing {symbol}, lpu: {last_price_update}, lfu: {last_finance_update}, lcu: {last_calendar_update}"
        )
        if (
                backfill
                or start_date <= today
                or self.data_saver.should_update(last_finance_update)
                or self.data_saver.should_update(last_calendar_update)
        ):
            data, info, yf_ticker = self.data_fetcher.fetch_stock_data(
                symbol, exchange, start_date, end_date
            )
            if data is not None and not data.empty:
                if self.data_saver.save_price_data(ticker_id, symbol, data):
                    updates.add("historical_prices")
            if (
                    backfill or self.data_saver.should_update(last_finance_update)
            ) and info:
                if self.data_saver.update_ticker_info(
                        ticker_id, symbol, info, backfill
                ):
                    updates.add("tickers")
                if self.data_saver.save_finance_data(ticker_id, symbol, info):
                    updates.add("yh_finance_daily")
                quote_type = info.get("quoteType", "EQUITY")
                if quote_type in ["MUTUALFUND", "ETF"] and yf_ticker:
                    updates.update(self.process_fund_data(ticker_id, symbol, yf_ticker))

            if (
                    backfill or self.data_saver.should_update(last_calendar_update)
            ) and yf_ticker:
                if self.data_saver.save_calendar_events(ticker_id, symbol, yf_ticker):
                    updates.add("calendar_events")

            # Now do stuff for recommnending dividends and reinvestments
        return updates

    def process_fund_data(self, ticker_id, symbol, yf_ticker):
        updates = set()
        if self.data_saver.save_fund_top_holdings(ticker_id, symbol, yf_ticker):
            updates.add("fund_top_holdings")
            logger.info(f"Updated fund_top_holdings for {symbol}")

        # Save fund sector weightings
        if self.data_saver.save_fund_sector_weightings(ticker_id, symbol, yf_ticker):
            updates.add("fund_sector_weightings")
            logger.info(f"Updated fund_sector_weightings for {symbol}")

        # Save fund asset classes
        if self.data_saver.save_fund_asset_classes(ticker_id, symbol, yf_ticker):
            updates.add("fund_asset_classes")
            logger.info(f"Updated fund_asset_classes for {symbol}")
        return updates
</file>

<file path="src/lambda_handler.py">
from typing import Dict, List
from pydantic import BaseModel
from src.core.config import SUPABASE_URL, SUPABASE_KEY
from src.core.supabase_client import SupabaseClient
from src.services.ticker_fetcher import TickerFetcher
from src.services.ticker_processor import TickerProcessor
from src.core.logging_config import setup_logging

logger = setup_logging(name="ticker_processor")


class ProcessingResult(BaseModel):
    ticker_count: int
    successful: List[str]
    failed: Dict[str, str]
    updated_tables: Dict[str, List[str]]


def initialize_components() -> tuple:
    """Initialize dependencies with error handling."""
    try:
        supabase = SupabaseClient(SUPABASE_URL, SUPABASE_KEY).get_client()
        return TickerFetcher(supabase), TickerProcessor(supabase)
    except Exception as e:
        logger.error("Failed to initialize components", extra={"error": str(e)})
        raise


def process_tickers(tickers: List[Dict], processor: TickerProcessor) -> ProcessingResult:
    """Process tickers with detailed tracking."""
    result = ProcessingResult(
        ticker_count=len(tickers),
        successful=[],
        failed={},
        updated_tables={}
    )

    for ticker in tickers:
        symbol = ticker["symbol"]
        try:
            logger.info(f"Starting processing for {symbol}", extra={"ticker_id": ticker["id"]})
            updates = processor.process_ticker(ticker)
            result.successful.append(symbol)
            result.updated_tables[symbol] = list(updates)
            logger.info(f"Completed processing {symbol}", extra={"updates": updates})
        except Exception as e:
            logger.error(f"Error processing {symbol}", extra={"error": str(e)})
            result.failed[symbol] = str(e)

    return result


def lambda_handler(event, context):
    """Main Lambda Entry Point """
    logger.info("Lambda execution started", extra={"event": event or {}})

    # Initializing
    try:
        ticker_fetcher, ticker_processor = initialize_components()
    except Exception as e:
        return {"statusCode": 500, "body": f"Initialization failed: {str(e)}"}

    # Fetching Tickers from Supabase
    tickers = ticker_fetcher.fetch_tickers()
    if not tickers:
        logger.warning("No tickers found in Supabase!")
        return {"statusCode": 400, "body": "No tickers found"}

        # Processing the Tickers
    result = process_tickers(tickers, ticker_processor)

    # Build and return Summary
    summary = (
            f"Processed {result.ticker_count} tickers\n"
            f"Successful: {len(result.successful)}\n"
            f"Failed: {len(result.failed)}\n"
            f"Details:\n" +
            "\n".join(
                f"{s}: {', '.join(result.updated_tables.get(s, ['No updates']))}"
                if s in result.successful else f"{s}: Failed - {result.failed[s]}"
                for s in (result.successful + list(result.failed.keys()))
            )
    )

    logger.info("Execution completed", extra={"summary": summary})

    return {
        "statusCode": 200 if not result.failed else 207,
        "body": summary
    }


if __name__ == "__main__":
    result = lambda_handler(None, None)
    print(result["body"])
</file>

<file path="tests/test_ticker_process.py">
import pytest
import pickle
from unittest.mock import Mock, patch
from src.services.ticker_processor import TickerProcessor
from src.services.data_fetcher import DataFetcher
from src.services.data_saver import DataSaver
from datetime import date

@pytest.fixture
def mock_supabase():
    return Mock()

@pytest.fixture
def ticker_processor(mock_supabase):
    return TickerProcessor(mock_supabase)

def load_yfinance_data(filename):
    with open(f"tests/{filename}", "rb") as f:
        cached_data = pickle.load(f)
    return (
        cached_data["history"],
        cached_data["info"],
        cached_data["calendar"],
        cached_data.get("funds_data")
    )

def test_process_ticker_aapl(ticker_processor):
    ticker = {"id": "aapl-id", "symbol": "AAPL", "exchange": "NASDAQ", "backfill": False}
    real_data, real_info, real_calendar, _ = load_yfinance_data("aapl_yfinance_data.pkl")

    mock_ticker = Mock()
    mock_ticker.history = Mock(return_value=real_data)
    mock_ticker.info = real_info
    mock_ticker.calendar = real_calendar

    with patch.object(DataFetcher, "fetch_stock_data", return_value=(real_data, real_info, mock_ticker)), \
         patch.object(DataSaver, "get_last_update_date", return_value=date(2025, 3, 16)), \
         patch.object(DataSaver, "save_price_data", return_value=1), \
         patch.object(DataSaver, "should_update", return_value=True), \
         patch.object(DataSaver, "update_ticker_info", return_value=True), \
         patch.object(DataSaver, "save_finance_data", return_value=True), \
         patch.object(DataSaver, "save_calendar_events", return_value=True):

        updates = ticker_processor.process_ticker(ticker)

        assert "historical_prices" in updates, "Price data should be updated"
        assert "tickers" in updates, "Ticker info should be updated"
        assert "yh_finance_daily" in updates, "Finance data should be updated"
        assert "calendar_events" in updates, "Calendar events should be updated"
        assert len(updates) == 4, "Expected exactly 4 updates"

        ticker_processor.data_saver.save_price_data.assert_called_once()
        ticker_processor.data_saver.update_ticker_info.assert_called_once()
        ticker_processor.data_saver.save_finance_data.assert_called_once()
        ticker_processor.data_saver.save_calendar_events.assert_called_once_with(
            "aapl-id", "AAPL", mock_ticker
        )

def test_process_ticker_spy(ticker_processor):
    ticker = {"id": "spy-id", "symbol": "SPY", "exchange": "NYSE", "backfill": False}
    real_data, real_info, real_calendar, real_funds_data = load_yfinance_data("spy_yfinance_data.pkl")

    mock_ticker = Mock()
    mock_ticker.history = Mock(return_value=real_data)
    mock_ticker.info = real_info
    mock_ticker.calendar = real_calendar
    mock_ticker.funds_data = Mock()
    if real_funds_data:
        mock_ticker.funds_data.description = real_funds_data["description"]
        mock_ticker.funds_data.fund_overview = real_funds_data["fund_overview"]
        mock_ticker.funds_data.fund_operations = real_funds_data["fund_operations"]
        mock_ticker.funds_data.asset_classes = real_funds_data["asset_classes"]
        mock_ticker.funds_data.top_holdings = real_funds_data["top_holdings"]
        mock_ticker.funds_data.equity_holdings = real_funds_data["equity_holdings"]
        mock_ticker.funds_data.bond_holdings = real_funds_data["bond_holdings"]
        mock_ticker.funds_data.bond_ratings = real_funds_data["bond_ratings"]
        mock_ticker.funds_data.sector_weightings = real_funds_data["sector_weightings"]

    with patch.object(DataFetcher, "fetch_stock_data", return_value=(real_data, real_info, mock_ticker)), \
         patch.object(DataSaver, "get_last_update_date", return_value=date(2025, 3, 16)), \
         patch.object(DataSaver, "save_price_data", return_value=1), \
         patch.object(DataSaver, "should_update", return_value=True), \
         patch.object(DataSaver, "update_ticker_info", return_value=True), \
         patch.object(DataSaver, "save_finance_data", return_value=True), \
         patch.object(DataSaver, "save_calendar_events", return_value=True), \
         patch.object(DataSaver, "save_fund_top_holdings", return_value=bool(real_funds_data and real_funds_data["top_holdings"] is not None)), \
         patch.object(DataSaver, "save_fund_sector_weightings", return_value=bool(real_funds_data and real_funds_data["sector_weightings"] is not None)), \
         patch.object(DataSaver, "save_fund_asset_classes", return_value=bool(real_funds_data and real_funds_data["asset_classes"] is not None)):

        updates = ticker_processor.process_ticker(ticker)

        assert "historical_prices" in updates, "Price data should be updated"
        assert "tickers" in updates, "Ticker info should be updated"
        assert "yh_finance_daily" in updates, "Finance data should be updated"
        assert "calendar_events" in updates, "Calendar events should be updated"

        expected_updates = 4
        if real_funds_data and real_funds_data["top_holdings"] is not None:
            assert "fund_top_holdings" in updates, "Fund top holdings should be updated"
            expected_updates += 1
        if real_funds_data and real_funds_data["sector_weightings"] is not None:
            assert "fund_sector_weightings" in updates, "Fund sector weightings should be updated"
            expected_updates += 1
        if real_funds_data and real_funds_data["asset_classes"] is not None:
            assert "fund_asset_classes" in updates, "Fund asset classes should be updated"
            expected_updates += 1

        assert len(updates) == expected_updates, f"Expected {expected_updates} updates"

        ticker_processor.data_saver.save_price_data.assert_called_once()
        ticker_processor.data_saver.update_ticker_info.assert_called_once()
        ticker_processor.data_saver.save_finance_data.assert_called_once()
        ticker_processor.data_saver.save_calendar_events.assert_called_once_with(
            "spy-id", "SPY", mock_ticker
        )
        if real_funds_data and real_funds_data["top_holdings"] is not None:
            ticker_processor.data_saver.save_fund_top_holdings.assert_called_once_with(
                "spy-id", "SPY", mock_ticker
            )
        if real_funds_data and real_funds_data["sector_weightings"] is not None:
            ticker_processor.data_saver.save_fund_sector_weightings.assert_called_once_with(
                "spy-id", "SPY", mock_ticker
            )
        if real_funds_data and real_funds_data["asset_classes"] is not None:
            ticker_processor.data_saver.save_fund_asset_classes.assert_called_once_with(
                "spy-id", "SPY", mock_ticker
            )

        assert real_info.get("quoteType") == "ETF", "SPY should be an ETF"
</file>

<file path=".gitignore">
# Distribution / packaging
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# File used for mock tests
tests/aapl_yfinance_data.pkl
tests/spy_yfinance_data.pkl
# Serverless directories
.serverless
</file>

<file path="fetch_yfinance_data.py">
# fetch_yfinance_data.py
import yfinance as yf
import pickle
from datetime import date
from dateutil.relativedelta import relativedelta

# Set date range (last 30 days up to today, March 18, 2025)
end_date = date(2025, 3, 18)
start_date = end_date - relativedelta(days=30)

def fetch_and_save_ticker(symbol, filename):
    yf_ticker = yf.Ticker(symbol)
    real_data = yf_ticker.history(start=start_date, end=end_date, auto_adjust=True)
    real_info = yf_ticker.info
    real_calendar = yf_ticker.calendar

    # Fetch funds_data for ETFs/Mutual Funds
    funds_data = {}
    if real_info.get("quoteType") in ["ETF", "MUTUALFUND"]:
        fund_data = yf_ticker.funds_data
        funds_data = {
            "description": fund_data.description,
            "fund_overview": fund_data.fund_overview,
            "fund_operations": fund_data.fund_operations,
            "asset_classes": fund_data.asset_classes,
            "top_holdings": fund_data.top_holdings,
            "equity_holdings": fund_data.equity_holdings,
            "bond_holdings": fund_data.bond_holdings,
            "bond_ratings": fund_data.bond_ratings,
            "sector_weightings": fund_data.sector_weightings
        }

    # Save to a file
    with open(f"tests/{filename}", "wb") as f:
        pickle.dump({
            "history": real_data,
            "info": real_info,
            "calendar": real_calendar,
            "funds_data": funds_data if funds_data else None
        }, f)

    print(f"Saved {symbol} data: {start_date} to {end_date}")
    print(f"History shape: {real_data.shape}")
    print(f"Info keys: {list(real_info.keys())}")
    print(f"Calendar: {real_calendar}")
    if funds_data:
        print(f"Funds data keys: {list(funds_data.keys())}")
        for key, value in funds_data.items():
            print(f"{key}: {type(value)} - {value}")

# Fetch data for AAPL (EQUITY) and SPY (ETF)
fetch_and_save_ticker("AAPL", "aapl_yfinance_data.pkl")
fetch_and_save_ticker("SPY", "spy_yfinance_data.pkl")
</file>

<file path="package.json">
{
  "devDependencies": {
    "serverless-python-requirements": "^6.1.2"
  }
}
</file>

<file path="pytest.ini">
[pytest]
pythonpath = .
</file>

<file path="README.md">
# Daily Market Update Lambda Function

The `daily-market-update` is a serverless AWS Lambda function designed to fetch financial market data for a list of tickers from Yahoo Finance and store it in a Supabase database. It runs daily at 5:30 PM Eastern Time (ET) after the NYSE market close and at midnight ET, ensuring up-to-date financial data for analysis and tracking.

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Features](#features)
4. [Prerequisites](#prerequisites)
5. [Setup](#setup)
6. [Deployment](#deployment)
7. [Environment Variables](#environment-variables)
8. [Scheduling](#scheduling)
9. [File Structure](#file-structure)
10. [Usage](#usage)
11. [Troubleshooting](#troubleshooting)
12. [Contributing](#contributing)
13. [License](#license)

## Overview

This project automates the retrieval and storage of stock market data, including historical prices, daily finance metrics, ticker information, calendar events (e.g., earnings, dividends), and fund-specific data (e.g., sector weightings, top holdings). It leverages the Serverless Framework for deployment, Yahoo Finance (`yfinance`) for data retrieval, and Supabase as the database backend.

The Lambda function is triggered by scheduled events, processes tickers stored in the `tickers` table, and updates multiple related tables in Supabase based on the fetched data.

## Architecture

- **AWS Lambda**: Executes the core logic via `lambda_handler.py`.
- **Serverless Framework**: Manages deployment and configuration (`serverless.yml`).
- **Supabase**: Stores ticker data and fetched financial information.
- **Yahoo Finance**: Provides market data through the `yfinance` Python library.
- **AWS SSM**: Securely stores environment variables (Supabase URL and key).
- **Cron Schedules**: Triggers the function at 5:30 PM ET and midnight ET daily.

The codebase is modular, with separate classes for fetching tickers, retrieving data, and saving it to the database.

## Features

- Fetches historical price data, daily finance metrics, ticker info, calendar events, and fund-specific data.
- Supports backfill mode to retrieve data from January 1, 2020, for initial setup or missing history.
- Handles multiple exchanges (e.g., NYSE, NASDAQ, TSX) with appropriate ticker suffixes.
- Updates data incrementally based on the last update date, minimizing redundant fetches.
- Logs execution details for debugging and monitoring.
- Runs on a schedule tailored to Eastern Time, accounting for DST.

## Prerequisites

- **AWS Account**: With permissions to create Lambda functions, IAM roles, and SSM parameters.
- **Serverless Framework**: Installed globally (`npm install -g serverless`).
- **Node.js**: For managing Serverless plugins (version compatible with `serverless-python-requirements`).
- **Python 3.10**: Runtime environment for the Lambda function.
- **AWS CLI**: Configured with a profile (e.g., `my-dashboard`).
- **Supabase Account**: With a project set up and tables created (see schema in `data_saver.py`).

## Setup

1. **Clone the Repository**:

   ```bash
   git clone <repository-url>
   cd daily-market-update
   ```

2. **Install Dependencies**:

   - Install Node.js dependencies:

     ```bash
     npm install
     ```

   - Install Python dependencies:

     ```bash
     pip install -r requirements.txt
     ```

3. **Configure AWS CLI**:
   Ensure your AWS CLI is configured with the `my-dashboard` profile:

   ```bash
   aws configure --profile my-dashboard
   ```

4. **Set Up Supabase**:

   - Create a Supabase project and note the URL and API key.
   - Set up the required tables (`tickers`, `historical_prices`, `yh_finance_daily`, `calendar_events`, `fund_sector_weightings`, `fund_top_holdings`, `fund_asset_classes`) with appropriate schemas (infer from `data_saver.py`).

5. **Store Secrets in AWS SSM**:
   Replace placeholders with your Supabase credentials:

   ```bash
   aws ssm put-parameter --name "/daily-market-update/supabase-url" --value "https://your-supabase-url.supabase.co" --type SecureString --region us-east-1 --profile my-dashboard
   aws ssm put-parameter --name "/daily-market-update/supabase-key" --value "your-supabase-key" --type SecureString --region us-east-1 --profile my-dashboard
   ```

## Deployment

Deploy the function to AWS using the Serverless Framework:

```bash
serverless deploy --aws-profile my-dashboard
```

- This deploys to the `dev` stage in `us-east-1`.
- The deployment creates a Lambda function (`processTickerData`), CloudWatch Events rules, and an IAM role with SSM access.

To remove the deployment:

```bash
serverless remove --aws-profile my-dashboard
```

To test locally:

```bash
serverless invoke local -f processTickerData
```

## Environment Variables

The function uses the following environment variables, sourced from AWS SSM:

- `SUPABASE_URL`: URL of your Supabase instance (e.g., `https://your-supabase-url.supabase.co`).
- `SUPABASE_KEY`: Supabase API key for authentication.

These are automatically injected via the `serverless.yml` configuration:

```yaml
environment:
  SUPABASE_URL: ${ssm:/daily-market-update/supabase-url}
  SUPABASE_KEY: ${ssm:/daily-market-update/supabase-key}
```

## Scheduling

The function runs on two daily schedules, adjusted for Eastern Time (ET) with DST:

- **5:30 PM ET (Post-Market Close)**:
  - EDT (UTC-4): `cron(30 21 ? * MON-SAT *)` (9:30 PM UTC).
  - EST (UTC-5): `cron(30 22 ? * MON-SAT *)` (10:30 PM UTC).
  - Runs Monday to Saturday, after NYSE close at 4:00 PM ET.
- **12:00 AM ET (Midnight)**:
  - EDT (UTC-4): `cron(0 4 ? * MON-SUN *)` (4:00 AM UTC).
  - EST (UTC-5): `cron(0 5 ? * MON-SUN *)` (5:00 AM UTC).
  - Runs daily.

These schedules are defined in `serverless.yml` under the `processTickerData` function's `events`.

## File Structure

```
daily-market-update/
 .gitignore              # Excludes build artifacts, env files, etc.
 config.py               # Loads Supabase credentials from environment
 data_fetcher.py         # Fetches market data from Yahoo Finance
 data_saver.py           # Saves data to Supabase tables
 handler.py              # Sample Lambda handler (unused)
 lambda_handler.py       # Main Lambda entry point
 package.json            # Node.js dependencies (Serverless plugins)
 README.md               # Project documentation
 requirements.txt        # Python dependencies
 serverless.yml          # Serverless Framework configuration
 supabase_client.py      # Supabase client wrapper
 ticker_fetcher.py       # Fetches tickers from Supabase
 ticker_processor.py     # Processes ticker data
```

## Usage

1. **Populate the `tickers` Table**:

   - Add ticker entries to the `tickers` table in Supabase with columns: `id` (UUID), `symbol` (string), `exchange` (string, optional), `backfill` (boolean).
   - Example:

     ```sql
     INSERT INTO tickers (id, symbol, exchange, backfill)
     VALUES ('550e8400-e29b-41d4-a716-446655440000', 'AAPL', 'NASDAQ', true);
     ```

2. **Function Execution**:

   - The Lambda runs automatically at 5:30 PM ET and midnight ET.
   - For each ticker, it:
     - Fetches data from Yahoo Finance.
     - Updates `historical_prices`, `yh_finance_daily`, `tickers`, `calendar_events`, and fund tables as needed.
     - Logs progress and errors.

3. **Monitoring**:
   - Check CloudWatch Logs for execution details (under log group `/aws/lambda/daily-market-update-dev-processTickerData`).
   - Verify data updates in Supabase tables.

## Troubleshooting

- **Deployment Fails**: Ensure AWS credentials are valid and SSM parameters are set.
- **No Data Updates**: Confirm `tickers` table has entries and Yahoo Finance is accessible.
- **DST Issues**: Schedules may overlap briefly during DST transitions; this is benign unless precision is critical.
- **Timeout Errors**: Increase `timeout` in `serverless.yml` (currently 300 seconds) if processing many tickers.
- **Logs**: Enable debug logging by setting `logger.setLevel(logging.DEBUG)` in `lambda_handler.py` temporarily.
</file>

<file path="requirements.txt">
yfinance
supabase
python-dotenv
dotenv
pandas
pydantic==2.4.2
pydantic-core==2.10.1
</file>

<file path="serverless.yml">
service: daily-market-update
org: dappner
app: daily-market-update
frameworkVersion: "3"

provider:
  name: aws
  runtime: python3.10
  region: us-east-1
  profile: my-dashboard
  environment:
    SUPABASE_URL: ${ssm:/daily-market-update/supabase-url}
    SUPABASE_KEY: ${ssm:/daily-market-update/supabase-key}
  iam:
    role:
      statements:
        - Effect: Allow
          Action:
            - ssm:GetParameter
          Resource: "arn:aws:ssm:${aws:region}:${aws:accountId}:parameter/daily-market-update/*"

plugins:
  - serverless-python-requirements

package:
  individually: true
  patterns:
    - "!node_modules/**"
    - "!tests/**"
    - "!__pycache__/**"
    - "!.pytest_cache/**"
    - "!.git/**"
    - "!.github/**"
    - "!.vscode/**"
    - "!*.dist-info/**"
    - "!*.egg-info/**"

functions:
  processTickerData:
    handler: src.lambda_handler.lambda_handler
    events:
      # 6:00 AM Eastern Time
      - schedule:
          # 6:00 AM EDT = 10:00 UTC (March to November)
          rate: cron(0 10 ? * MON-SUN *)
          enabled: true
      - schedule:
          # 6:00 AM EST = 11:00 UTC (November to March)
          rate: cron(0 11 ? * MON-SUN *)
          enabled: true
      # 12:00 PM Eastern Time (Noon)
      - schedule:
          # 12:00 PM EDT = 16:00 UTC (March to November)
          rate: cron(0 16 ? * MON-SUN *)
          enabled: true
      - schedule:
          # 12:00 PM EST = 17:00 UTC (November to March)
          rate: cron(0 17 ? * MON-SUN *)
          enabled: true
      # 5:30 PM Eastern Time (after NYSE close)
      - schedule:
          # 5:30 PM EDT = 21:30 UTC (March to November)
          rate: cron(30 21 ? * MON-FRI *)
          enabled: true
      - schedule:
          # 5:30 PM EST = 22:30 UTC (November to March)
          rate: cron(30 22 ? * MON-FRI *)
          enabled: true
      # 12:00 AM Eastern Time (Midnight)
      - schedule:
          # 12:00 AM EDT = 04:00 UTC (March to November)
          rate: cron(0 4 ? * MON-SUN *)
          enabled: true
      - schedule:
          # 12:00 AM EST = 05:00 UTC (November to March)
          rate: cron(0 5 ? * MON-SUN *)
          enabled: true
    timeout: 300 # 5 minutes
    memorySize: 512
</file>

<file path="test.py">
import yfinance as yf
from datetime import date
from dateutil.relativedelta import relativedelta


end_date = date(2025, 3, 20)
start_date = end_date - relativedelta(days=5)


symbol = "TAP"
yf_ticker = yf.Ticker(symbol)
real_data = yf_ticker.history(start=start_date, end=end_date, auto_adjust=True)

print(real_data)
</file>

</files>
